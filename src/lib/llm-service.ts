import { LLMConfig, LLMMessage, LLMResponse, LLMStreamChunk, LLMProviderAdapter, LLMProvider } from '@/types/llm';
import OpenAIAdapter from './adapters/openai-adapter';
import AnthropicAdapter from './adapters/anthropic-adapter';
import DeepSeekAdapter from './adapters/deepseek-adapter';
import DoubaoAdapter from './adapters/doubao-adapter';
import GoogleAdapter from './adapters/google-adapter';
import XAIAdapter from './adapters/xai-adapter';
import ModelScopeAdapter from './adapters/modelscope-adapter-simple';
import BigModelAdapter from './adapters/bigmodel-adapter';

class LLMService {
  private adapters: Map<LLMProvider, LLMProviderAdapter> = new Map();

  constructor() {
    // æ³¨å†Œæ‰€æœ‰é€‚é…å™¨
    this.adapters.set('openai', new OpenAIAdapter());
    this.adapters.set('anthropic', new AnthropicAdapter());
    this.adapters.set('deepseek', new DeepSeekAdapter());
    this.adapters.set('doubao', new DoubaoAdapter());
    this.adapters.set('google', new GoogleAdapter());
    this.adapters.set('xai', new XAIAdapter());
    this.adapters.set('modelscope', new ModelScopeAdapter());
    this.adapters.set('bigmodel', new BigModelAdapter());
  }

  /**
   * è·å–é€‚é…å™¨
   */
  private getAdapter(provider: LLMProvider): LLMProviderAdapter {
    const adapter = this.adapters.get(provider);
    if (!adapter) {
      throw new Error(`Unsupported LLM provider: ${provider}`);
    }
    return adapter;
  }

  /**
   * æµå¼èŠå¤©
   */
  async streamChat(
    config: LLMConfig,
    messages: LLMMessage[],
    onChunk: (chunk: LLMStreamChunk) => void
  ): Promise<LLMResponse> {
    console.log(`ğŸš€ [LLMService] streamChat called with config:`, {
      provider: config.provider,
      providerType: typeof config.provider,
      model: config.model,
      hasApiKey: !!config.apiKey,
      baseURL: config.baseURL
    });
    
    const adapter = this.getAdapter(config.provider);
    
    // éªŒè¯é…ç½®
    if (!adapter.validateConfig(config)) {
      throw new Error(`Invalid configuration for provider: ${config.provider}`);
    }

    return adapter.streamChat(config, messages, onChunk);
  }

  /**
   * éæµå¼èŠå¤©
   */
  async chat(config: LLMConfig, messages: LLMMessage[]): Promise<LLMResponse> {
    const adapter = this.getAdapter(config.provider);
    
    // éªŒè¯é…ç½®
    if (!adapter.validateConfig(config)) {
      throw new Error(`Invalid configuration for provider: ${config.provider}`);
    }

    return adapter.chat(config, messages);
  }

  /**
   * éªŒè¯é…ç½®
   */
  validateConfig(config: LLMConfig): boolean {
    try {
      const adapter = this.getAdapter(config.provider);
      return adapter.validateConfig(config);
    } catch {
      return false;
    }
  }

  /**
   * è·å–å¯ç”¨æ¨¡å‹
   */
  async getAvailableModels(provider: LLMProvider, apiKey: string): Promise<string[]> {
    const adapter = this.getAdapter(provider);
    return adapter.getAvailableModels(apiKey);
  }

  /**
   * è·å–æ‰€æœ‰æ”¯æŒçš„æä¾›å•†
   */
  getSupportedProviders(): LLMProvider[] {
    return Array.from(this.adapters.keys());
  }

  /**
   * è®¡ç®—æˆæœ¬ï¼ˆç¾åˆ†ï¼‰
   */
  calculateCost(provider: LLMProvider, model: string, usage: { promptTokens: number; completionTokens: number }): number {
    // è¿™é‡Œå¯ä»¥ä»é…ç½®æ–‡ä»¶è¯»å–ä»·æ ¼ä¿¡æ¯
    // æš‚æ—¶è¿”å›0ï¼Œå®é™…å®ç°æ—¶ä» llm-providers.json è¯»å–
    return 0;
  }
}

// å•ä¾‹å®ä¾‹
export const llmService = new LLMService();
export default llmService;
