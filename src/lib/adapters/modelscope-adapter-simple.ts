import { LLMConfig, LLMMessage, LLMResponse, LLMStreamChunk, LLMProvider } from '@/types/llm';
import OpenAIAdapter from './openai-adapter';

/**
 * é­”æ­ç¤¾åŒºé€‚é…å™¨ï¼ˆå¢å¼ºç‰ˆï¼‰
 * ç»§æ‰¿ OpenAI é€‚é…å™¨ï¼Œå¢åŠ äº†rate limitingå¤„ç†å’Œé‡è¯•æœºåˆ¶
 */
export default class ModelScopeAdapter extends OpenAIAdapter {
  provider: LLMProvider = 'modelscope';
  private retryCount = 0;
  private maxRetries = 2;
  private baseDelay = 2000; // 2ç§’åŸºç¡€å»¶è¿Ÿ

  async getAvailableModels(apiKey: string): Promise<string[]> {
    // é­”æ­å¸¸ç”¨æ¨¡å‹åˆ—è¡¨
    return [
      'deepseek-ai/DeepSeek-V3.1'
    ];
  }

  // é‡å†™ streamChat ä»¥ä½¿ç”¨æ•°æ®åº“é…ç½®çš„ç«¯ç‚¹ï¼Œå¹¶å¢åŠ é‡è¯•æœºåˆ¶
  async streamChat(
    config: LLMConfig,
    messages: LLMMessage[],
    onChunk: (chunk: LLMStreamChunk) => void
  ): Promise<LLMResponse> {
    // ä½¿ç”¨æ•°æ®åº“é…ç½®çš„baseUrlï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤çš„ModelScopeç¤¾åŒºç‰ˆç«¯ç‚¹
    const modifiedConfig = {
      ...config,
      baseUrl: config.baseUrl || 'https://api-inference.modelscope.cn/v1'
    };

    console.log(`ğŸ”§ [ModelScopeAdapter] Using baseUrl: ${modifiedConfig.baseUrl}`);
    
    return this.executeWithRetry(
      () => super.streamChat(modifiedConfig, messages, onChunk),
      'streamChat'
    );
  }

  // é‡å†™ chat ä»¥ä½¿ç”¨æ•°æ®åº“é…ç½®çš„ç«¯ç‚¹ï¼Œå¹¶å¢åŠ é‡è¯•æœºåˆ¶
  async chat(config: LLMConfig, messages: LLMMessage[]): Promise<LLMResponse> {
    // ä½¿ç”¨æ•°æ®åº“é…ç½®çš„baseUrlï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤çš„ModelScopeç¤¾åŒºç‰ˆç«¯ç‚¹
    const modifiedConfig = {
      ...config,
      baseUrl: config.baseUrl || 'https://api-inference.modelscope.cn/v1'
    };

    return this.executeWithRetry(
      () => super.chat(modifiedConfig, messages),
      'chat'
    );
  }

  /**
   * å¸¦é‡è¯•æœºåˆ¶çš„æ‰§è¡Œå™¨ - ä¸“é—¨å¤„ç†rate limiting
   */
  private async executeWithRetry<T>(
    operation: () => Promise<T>,
    operationName: string
  ): Promise<T> {
    let lastError: Error;
    
    for (let attempt = 0; attempt <= this.maxRetries; attempt++) {
      try {
        if (attempt > 0) {
          const delay = this.baseDelay * Math.pow(2, attempt - 1); // æŒ‡æ•°é€€é¿
          console.log(`â³ [ModelScopeAdapter] Rate limit hit, waiting ${delay}ms before retry ${attempt}/${this.maxRetries}...`);
          await this.sleep(delay);
        }

        const result = await operation();
        
        // æˆåŠŸæ—¶é‡ç½®é‡è¯•è®¡æ•°
        this.retryCount = 0;
        return result;
        
      } catch (error) {
        lastError = error as Error;
        
        // æ£€æŸ¥æ˜¯å¦æ˜¯429é”™è¯¯ï¼ˆrate limitï¼‰
        if (this.isRateLimitError(error)) {
          console.log(`ğŸš« [ModelScopeAdapter] Rate limit detected in ${operationName}, attempt ${attempt + 1}/${this.maxRetries + 1}`);
          
          if (attempt < this.maxRetries) {
            continue; // ç»§ç»­é‡è¯•
          } else {
            // è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¿”å›å‹å¥½é”™è¯¯æ¶ˆæ¯
            throw new Error(`ğŸš« é­”æ­APIè¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åå†è¯•ã€‚å»ºè®®ï¼šç­‰å¾…2-3åˆ†é’Ÿåé‡æ–°æµ‹è¯•ï¼Œæˆ–è€ƒè™‘é…ç½®å…¶ä»–AIæä¾›å•†ä½œä¸ºå¤‡é€‰ã€‚åŸå§‹é”™è¯¯: ${lastError.message}`);
          }
        } else {
          // érate limité”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
          throw error;
        }
      }
    }
    
    throw lastError!;
  }

  /**
   * æ£€æŸ¥æ˜¯å¦æ˜¯rate limité”™è¯¯
   */
  private isRateLimitError(error: any): boolean {
    const errorMessage = error?.message?.toLowerCase() || '';
    const errorString = error?.toString?.()?.toLowerCase() || '';
    
    return (
      errorMessage.includes('429') ||
      errorMessage.includes('rate limit') ||
      errorMessage.includes('request limit exceeded') ||
      errorString.includes('429') ||
      errorString.includes('rate limit') ||
      errorString.includes('request limit exceeded')
    );
  }

  /**
   * å»¶è¿Ÿå·¥å…·å‡½æ•°
   */
  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}
