import { LLMConfig, LLMMessage, LLMResponse, LLMStreamChunk, LLMProvider } from '@/types/llm';
import OpenAIAdapter from './openai-adapter';

/**
 * é­”æ­ç¤¾åŒºé€‚é…å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
 * ç”±äºé­”æ­å®Œå…¨å…¼å®¹ OpenAI APIï¼Œç›´æ¥ç»§æ‰¿ OpenAI é€‚é…å™¨
 */
export default class ModelScopeAdapter extends OpenAIAdapter {
  provider: LLMProvider = 'modelscope';

  async getAvailableModels(apiKey: string): Promise<string[]> {
    // é­”æ­å¸¸ç”¨æ¨¡å‹åˆ—è¡¨
    return [
      'deepseek-ai/DeepSeek-V3.1'
    ];
  }

  // é‡å†™ streamChat ä»¥ä½¿ç”¨æ•°æ®åº“é…ç½®çš„ç«¯ç‚¹
  async streamChat(
    config: LLMConfig,
    messages: LLMMessage[],
    onChunk: (chunk: LLMStreamChunk) => void
  ): Promise<LLMResponse> {
    // ä½¿ç”¨æ•°æ®åº“é…ç½®çš„baseUrlï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤çš„ModelScopeç¤¾åŒºç‰ˆç«¯ç‚¹
    const modifiedConfig = {
      ...config,
      baseUrl: config.baseUrl || 'https://api-inference.modelscope.cn/v1'
    };

    console.log(`ğŸ”§ [ModelScopeAdapter] Using baseUrl: ${modifiedConfig.baseUrl}`);
    
    // è°ƒç”¨çˆ¶ç±»çš„ streamChat æ–¹æ³•
    return super.streamChat(modifiedConfig, messages, onChunk);
  }

  // é‡å†™ chat ä»¥ä½¿ç”¨æ•°æ®åº“é…ç½®çš„ç«¯ç‚¹
  async chat(config: LLMConfig, messages: LLMMessage[]): Promise<LLMResponse> {
    // ä½¿ç”¨æ•°æ®åº“é…ç½®çš„baseUrlï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤çš„ModelScopeç¤¾åŒºç‰ˆç«¯ç‚¹
    const modifiedConfig = {
      ...config,
      baseUrl: config.baseUrl || 'https://api-inference.modelscope.cn/v1'
    };

    // è°ƒç”¨çˆ¶ç±»çš„ chat æ–¹æ³•
    return super.chat(modifiedConfig, messages);
  }
}
