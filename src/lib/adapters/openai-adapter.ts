import { LLMConfig, LLMMessage, LLMResponse, LLMStreamChunk, LLMProvider } from '@/types/llm';
import { BaseLLMAdapter } from './base-adapter';

export default class OpenAIAdapter extends BaseLLMAdapter {
  provider: LLMProvider = 'openai';

  async streamChat(
    config: LLMConfig,
    messages: LLMMessage[],
    onChunk: (chunk: LLMStreamChunk) => void
  ): Promise<LLMResponse> {
    const url = `${config.baseUrl || 'https://api.openai.com/v1'}/chat/completions`;
    
    const requestBody = {
      model: config.model,
      messages: this.formatMessages(messages),
      temperature: config.temperature || 0.7,
      max_tokens: config.maxTokens || 2000,
      stream: true,
    };

    return this.withRetry(async () => {
      // ğŸ” å¼€å‘ç¯å¢ƒæ—¥å¿—ï¼šè¯·æ±‚å‚æ•°
      if (process.env.NODE_ENV === 'development') {
        console.log('ğŸš€ LLM API è¯·æ±‚å¼€å§‹:');
        console.log('ğŸ“ URL:', url);
        console.log('ğŸ”‘ Headers:', {
          ...this.buildHeaders(config),
          'Authorization': `Bearer ${config.apiKey.slice(0, 10)}...` // éšè—å®Œæ•´å¯†é’¥
        });
        console.log('ğŸ“¦ Request Body:', JSON.stringify(requestBody, null, 2));
      }

      const startTime = Date.now();
      const response = await fetch(url, {
        method: 'POST',
        headers: this.buildHeaders(config),
        body: JSON.stringify(requestBody),
      });

      const responseTime = Date.now() - startTime;

      if (!response.ok) {
        const errorText = await response.text();
        if (process.env.NODE_ENV === 'development') {
          console.error('âŒ LLM API é”™è¯¯:');
          console.error('ğŸ“ URL:', url);
          console.error('ğŸ”¢ Status:', response.status);
          console.error('ğŸ“„ Error:', errorText);
        }
        throw new Error(`OpenAI API Error: ${response.status} - ${errorText}`);
      }

      if (process.env.NODE_ENV === 'development') {
        console.log('âœ… LLM API å“åº”æˆåŠŸ:');
        console.log('â±ï¸ å“åº”æ—¶é—´:', `${responseTime}ms`);
        console.log('ğŸ“Š Status:', response.status);
        console.log('ğŸ”„ å¼€å§‹å¤„ç†æµå¼å“åº”...');
      }

      return this.processStream(response, onChunk);
    }, config.retryAttempts || 3);
  }

  async chat(config: LLMConfig, messages: LLMMessage[]): Promise<LLMResponse> {
    const url = `${config.baseUrl || 'https://api.openai.com/v1'}/chat/completions`;
    
    const requestBody = {
      model: config.model,
      messages: this.formatMessages(messages),
      temperature: config.temperature || 0.7,
      max_tokens: config.maxTokens || 2000,
      stream: false,
    };

    return this.withRetry(async () => {
      // ğŸ” å¼€å‘ç¯å¢ƒæ—¥å¿—ï¼šè¯·æ±‚å‚æ•°
      if (process.env.NODE_ENV === 'development') {
        console.log('ğŸš€ LLM API è¯·æ±‚å¼€å§‹ (éæµå¼):');
        console.log('ğŸ“ URL:', url);
        console.log('ğŸ”‘ Headers:', {
          ...this.buildHeaders(config),
          'Authorization': `Bearer ${config.apiKey.slice(0, 10)}...`
        });
        console.log('ğŸ“¦ Request Body:', JSON.stringify(requestBody, null, 2));
      }

      const startTime = Date.now();
      const response = await fetch(url, {
        method: 'POST',
        headers: this.buildHeaders(config),
        body: JSON.stringify(requestBody),
      });

      const responseTime = Date.now() - startTime;

      if (!response.ok) {
        const errorText = await response.text();
        if (process.env.NODE_ENV === 'development') {
          console.error('âŒ LLM API é”™è¯¯:');
          console.error('ğŸ“ URL:', url);
          console.error('ğŸ”¢ Status:', response.status);
          console.error('ğŸ“„ Error:', errorText);
        }
        throw new Error(`OpenAI API Error: ${response.status} - ${errorText}`);
      }

      const data = await response.json();

      if (process.env.NODE_ENV === 'development') {
        console.log('âœ… LLM API å“åº”æˆåŠŸ:');
        console.log('â±ï¸ å“åº”æ—¶é—´:', `${responseTime}ms`);
        console.log('ğŸ“Š Status:', response.status);
        console.log('ğŸ“„ Response Data:', JSON.stringify(data, null, 2));
        console.log('ğŸ’° Token ä½¿ç”¨:', {
          prompt: data.usage?.prompt_tokens || 0,
          completion: data.usage?.completion_tokens || 0,
          total: data.usage?.total_tokens || 0
        });
      }
      
      return {
        content: data.choices[0].message.content,
        usage: this.parseUsage(data.usage),
        model: data.model,
        finishReason: data.choices[0].finish_reason || 'stop'
      };
    }, config.retryAttempts || 3);
  }

  async getAvailableModels(apiKey: string): Promise<string[]> {
    const url = 'https://api.openai.com/v1/models';
    
    const response = await fetch(url, {
      headers: {
        'Authorization': `Bearer ${apiKey}`,
      },
    });

    if (!response.ok) {
      throw new Error(`OpenAI API Error: ${response.status}`);
    }

    const data = await response.json();
    return data.data
      .filter((model: any) => model.id.includes('gpt'))
      .map((model: any) => model.id);
  }

  private async processStream(
    response: Response,
    onChunk: (chunk: LLMStreamChunk) => void
  ): Promise<LLMResponse> {
    const reader = response.body?.getReader();
    if (!reader) {
      throw new Error('No response body');
    }

    const decoder = new TextDecoder();
    let content = '';
    let usage = { promptTokens: 0, completionTokens: 0, totalTokens: 0 };
    let model = '';
    let finishReason: 'stop' | 'length' | 'error' = 'stop';
    let chunkCount = 0;
    const startTime = Date.now();

    try {
      while (true) {
        const { done, value } = await reader.read();
        
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(line => line.trim() !== '');

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            
            if (data === '[DONE]') {
              if (process.env.NODE_ENV === 'development') {
                const totalTime = Date.now() - startTime;
                console.log('ğŸ æµå¼å“åº”å®Œæˆ:');
                console.log('ğŸ“Š æ€»å—æ•°:', chunkCount);
                console.log('ğŸ“ æ€»å†…å®¹é•¿åº¦:', content.length);
                console.log('â±ï¸ æ€»è€—æ—¶:', `${totalTime}ms`);
                console.log('ğŸ’° æœ€ç»ˆ Token ä½¿ç”¨:', usage);
              }
              onChunk({ content: '', isComplete: true, usage });
              break;
            }

            try {
              const parsed = JSON.parse(data);
              const delta = parsed.choices?.[0]?.delta;
              
              if (delta?.content) {
                content += delta.content;
                chunkCount++;
                
                if (process.env.NODE_ENV === 'development' && chunkCount <= 3) {
                  console.log(`ğŸ”„ æµå¼å— #${chunkCount}:`, {
                    content: delta.content,
                    length: delta.content.length
                  });
                } else if (process.env.NODE_ENV === 'development' && chunkCount === 4) {
                  console.log('ğŸ“ åç»­å—å°†ä¸å†æ˜¾ç¤ºè¯¦æƒ…...');
                }
                
                onChunk({ 
                  content: delta.content, 
                  isComplete: false 
                });
              }

              if (parsed.model) {
                model = parsed.model;
              }

              if (parsed.choices?.[0]?.finish_reason) {
                finishReason = parsed.choices[0].finish_reason;
                if (process.env.NODE_ENV === 'development') {
                  console.log('ğŸ›‘ å®ŒæˆåŸå› :', finishReason);
                }
              }

              if (parsed.usage) {
                usage = this.parseUsage(parsed.usage);
              }
            } catch (e) {
              if (process.env.NODE_ENV === 'development') {
                console.warn('âš ï¸ è§£ææµå¼æ•°æ®å¤±è´¥:', data.slice(0, 100));
              }
            }
          }
        }
      }
    } finally {
      reader.releaseLock();
    }

    return {
      content,
      usage,
      model,
      finishReason
    };
  }
}
